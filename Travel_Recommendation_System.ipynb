{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "colab_type": "code",
        "id": "gL7edKIZaysP",
        "outputId": "c1cc91e3-135b-4e2e-96fd-73ba3f675ca8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.26.0.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords #common words to filter using data\n",
        "from sklearn.metrics.pairwise import linear_kernel #can be applied to calculate the similarity between different travel destinations or places based on their attributes or features\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer #converts collection of documents into matrix of words, coz ML data basically reprsented in amtrix form.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #used to analyze and extract important keywords and phrases from this text. \n",
        "from sklearn.decomposition import LatentDirichletAllocation #extract specific places like beach ,mountain\n",
        "from sklearn.metrics.pairwise import cosine_similarity #calculate the similarity between two destinations based on user ratings, location, or other attributes \n",
        "from sklearn.feature_extraction.text import CountVectorizer #word to matrix\n",
        "import re #regular expressions module\n",
        "import random #suggest random destinations at home page\n",
        "import cufflinks # creates user friendly charts and reports if we wanna track\n",
        "pd.options.display.max_columns = 30 # tell panda to show max 30 coloumns when printing data\n",
        "from IPython.core.interactiveshell import InteractiveShell # interactive features for working with python code\n",
        "import plotly.figure_factory as ff #interactive plots and charts\n",
        "cufflinks.go_offline() #can generate plots and visualizations without needing to go online\n",
        "cufflinks.set_config_file(world_readable=True, theme='solar') #allows generated plots to be read by others, solar is like color  scehemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "Er1N4nyrbNW2",
        "outputId": "804ddf98-5d0c-4b03-bd2f-4436da1a428b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 225 Travel places in the data\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('Travel_Data.csv',encoding=\"latin-1\") #uses pandas library for reading the file\n",
        "df.head()# quick overview of data\n",
        "print('We have', len(df), 'Travel places in the data') #total places in our dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "colab_type": "code",
        "id": "VQ5zznbRbj0t",
        "outputId": "c2770b16-0a0f-42a0-ea3f-d9bf1f22e05f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Type</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Luksentyre, Outer Hebrides, Scotland</td>\n",
              "      <td>Beach</td>\n",
              "      <td>At the end of a winding road on the wind-batte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dune Du Pilat, France</td>\n",
              "      <td>Beach</td>\n",
              "      <td>In the warmer months, Europeï¿½s tallest sand ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Ora Beach Indonesia</td>\n",
              "      <td>Beach</td>\n",
              "      <td>With a higgledy-piggledy 37,000-mile coastline...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Awaroa South Island New Zealand</td>\n",
              "      <td>Beach</td>\n",
              "      <td>Set deep in the heart of New Zealandï¿½s glori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Die Plast Walker Bay Nature Reserve South Africa</td>\n",
              "      <td>Beach</td>\n",
              "      <td>Donï¿½t be surprised to spot the occasional so...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Name   Type  \\\n",
              "0   1              Luksentyre, Outer Hebrides, Scotland  Beach   \n",
              "1   2                             Dune Du Pilat, France  Beach   \n",
              "2   3                               Ora Beach Indonesia  Beach   \n",
              "3   4                   Awaroa South Island New Zealand  Beach   \n",
              "4   5  Die Plast Walker Bay Nature Reserve South Africa  Beach   \n",
              "\n",
              "                                         Description  \n",
              "0  At the end of a winding road on the wind-batte...  \n",
              "1  In the warmer months, Europeï¿½s tallest sand ...  \n",
              "2  With a higgledy-piggledy 37,000-mile coastline...  \n",
              "3  Set deep in the heart of New Zealandï¿½s glori...  \n",
              "4  Donï¿½t be surprised to spot the occasional so...  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "colab_type": "code",
        "id": "J9B95iYFbq5_",
        "outputId": "b57f829c-e523-4e23-effc-43a3ae3f76f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 'Luksentyre, Outer Hebrides, Scotland', 'Beach',\n",
              "       'At the end of a winding road on the wind-battered west coast of the Isle of Harris, youï¿½ll find something to make your eyes pop. Luskentyre Beach is a paint splash of brilliant white, surrounded by almost impossibly azure water. On a sunny day, itï¿½s so unerringly blue you may find it hard to believe you are at a beach in Scotland at all. But regardless of its paradoxically tropical feel, itï¿½s at its most beautiful when eel-coloured storm clouds roll in and seas turn flinty, turning this dramatic landscape an eerie monochrome.'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex = df[df.index==0].values[0] #values is aNumPy array, all of same data type string.\n",
        "ex  #extracts first row of data and stores it in ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "colab_type": "code",
        "id": "-Z9IXf2EsQ6f",
        "outputId": "97e1af3a-925c-4583-d30f-167010515362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\shwen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') #common word like 'the', 'and', 'in that are filtered out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ahMSRhVAb9oD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning:\n",
            "\n",
            "invalid escape sequence '\\['\n",
            "\n",
            "<>:1: SyntaxWarning:\n",
            "\n",
            "invalid escape sequence '\\['\n",
            "\n",
            "C:\\Users\\shwen\\AppData\\Local\\Temp\\ipykernel_12548\\3055159535.py:1: SyntaxWarning:\n",
            "\n",
            "invalid escape sequence '\\['\n",
            "\n"
          ]
        }
      ],
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]') #identify these chars and replace them with space to clean the data\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') #remove characters which arent in the mentioned set\n",
        "STOPWORDS = set(stopwords.words('english')) #stopwords.words('english')) retrievs common english stopwords & filter them out\n",
        "\n",
        "def clean_text(text): #functionn which takes string argument ,string name=text\n",
        "    \"\"\"\n",
        "        text: a string  #provides purpose of function\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lower is inbuilt function which returns all lowercase words\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # split will sepratae words wheever spaces, then word for word iterates over\n",
        "    return text                                                             #and only includes the word if its not in stopwords set\n",
        "    \n",
        "df['Description_Clean'] = df['Description'].apply(clean_text)\n",
        "#applies cleanText to description coloumn,after cleaning assigns it to new \"Description_clean\" coloumn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XYXBS9EWxpmB"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of ['Name'] are in the columns\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12548\\3290784008.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description_Clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#transforms DC coloumn into dc matrix   # min_df will include all words irregardless of frequency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#indices variables creates index od all dataframes, convinient to look later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\shwen\\OneDrive\\Documents\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   5869\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5870\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5873\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mNone of \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m are in the columns\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5875\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5876\u001b[0m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: \"None of ['Name'] are in the columns\""
          ]
        }
      ],
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "df.set_index('Name',inplace = True)\n",
        "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.0, stop_words='english') \n",
        "tfidf_matrix = tf.fit_transform(df['Description_Clean'])    #transforms DC coloumn into dc matrix   # min_df will include all words irregardless of frequency\n",
        "indices = pd.Series(df.index) #indices variables creates index od all dataframes, convinient to look later\n",
        "\n",
        "def get_recommendations(user_arr): #function, has paramter user_arr likely to generate travel reccommendation\n",
        "  rec_places = [] #list for travel recommendation\n",
        "  vectors = [] #used for storing tfid vectors\n",
        "  idx = [] #indices of rec_places list\n",
        "  for u in user_arr: #iterates through all user_arr elements\n",
        "    idx.append(indices[indices == u].index[0]) #indices=u checks which element from indices series(early)= u in user_array, in[whole] creates series of previous condition ka equal elements\n",
        "  for i in idx:   #iterates through idx list                             #stores in the idx list\n",
        "    vectors.append(tfidf_matrix[i]) #This code retrieves the TF-IDF vector associated with the user data at index i \n",
        "                                    #in the tfidf_matrix and appends it to the vectors list.\n",
        "\n",
        "  avg_vec = sum(vectors)/len(vectors) #mean of vectors\n",
        "  avg_vec_sim = linear_kernel(avg_vec, tfidf_matrix)  #cosine similarity between average vector & each vector in matrix, result in sim variable\n",
        "\n",
        "  avg_vec_sim = avg_vec_sim.flatten() #1-d array\n",
        "\n",
        "  top_10_rec = pd.Series(avg_vec_sim).sort_values(ascending = False) #converts avg_vec_sim into pandas series, and sorts in descending order, highest similarity first\n",
        "  top_10_indexes = list(top_10_rec.iloc[1:len(user_arr)+11].index) #takes indexes from index 1 to index 10,list of indices is indexes of top10 locations\n",
        "  rec_places = []\n",
        "  for i in top_10_indexes: #iterate through top10 indexes\n",
        "        rec_places.append([list(df.index)[i], df['Type'][i]]) #in rec_places list append df.index=name of destination, type of destination\n",
        "\n",
        "  return rec_places #return recommendations to user\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "brF5Nx50tCrp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shwen\\AppData\\Local\\Temp\\ipykernel_12548\\3290784008.py:26: FutureWarning:\n",
            "\n",
            "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_arr = ['Grand Canyon Rim-to-Rim, Arizona'] \n",
        "rec_places = get_recommendations(user_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "A0ij4UdNzeRa"
      },
      "outputs": [],
      "source": [
        "rec_places = [i for i in rec_places if i[0] not in user_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "colab_type": "code",
        "id": "HT_ccfyh0YO9",
        "outputId": "f020d66c-735b-400f-8b07-7e4fa377bba0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Buckskin Gulch to Paria Canyon via Wire Pass Trailhead', 'Hiking/Mountain'],\n",
              " ['Great Wall, China', 'Historical Monument'],\n",
              " ['Teton Crest Trail Grand Teton National Park, Wyoming', 'Hiking/Mountain'],\n",
              " ['Greenstone Ridge Trail, Michigan', 'Hiking/Mountain'],\n",
              " ['Tour du Mont Blanc Switzerland, Italy, France', 'Hiking/Mountain'],\n",
              " ['Rainforests of Brunei', 'Adventure/Forest'],\n",
              " ['Wonderland Trail Mount Rainier National Park, Washington',\n",
              "  'Hiking/Mountain'],\n",
              " ['Kalalau Trail Kauai, Hawaii', 'Hiking/Mountain'],\n",
              " ['Kepler Track Fiordland National Park, New Zealand', 'Hiking/Mountain'],\n",
              " ['Superior Hiking Trail Duluth, Minnesota, to the Canadian Border',\n",
              "  'Hiking/Mountain'],\n",
              " ['Panorama Ridge Garibaldi Provincial Park, Canada', 'Hiking/Mountain']]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rec_places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hY7ZbOWm18dY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Travel_Recommendation_System.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
